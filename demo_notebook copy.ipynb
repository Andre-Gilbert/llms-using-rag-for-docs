{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo Notebook "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook serves the purpose of demonstrating how the Retriaval Augmented Generation improves the LLM code generation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM code creation without RAG and CoALA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we import the necessary libraries and create an agent object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and agent creation\n",
    "import pandas as pd\n",
    "from llms.agents.react import ReActAgent\n",
    "from llms.clients.gpt import GPTClient\n",
    "from llms.settings import settings\n",
    "\n",
    "client = GPTClient(\n",
    "    client_id=settings.CLIENT_ID,\n",
    "    client_secret=settings.CLIENT_SECRET,\n",
    "    auth_url=settings.AUTH_URL,\n",
    "    api_base=settings.API_BASE,\n",
    "    deployment_id=\"gpt-4-32k\",\n",
    "    max_response_tokens=1000,\n",
    "    temperature=0.0,\n",
    ")\n",
    "agent = ReActAgent(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define a prompt and ask the agent without RAG and CoALA implemented for the code. Then we execute the function code and save the function that the agent generated under `agent_func`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"How can I convert this one-hot-encoded dataframe: df = pd.DataFrame({\"col1_a\": [1, 0, 1], \"col1_b\": [0, 1, 0], \"col2_a\": [0, 1, 0], \"col2_b\": [1, 0, 0], \"col2_c\": [0, 0, 1]})\n",
    "into a categorical dataframe?\"\"\"\n",
    "\n",
    "generated_code = agent.run(prompt)\n",
    "\n",
    "namespace_agent = {}\n",
    "exec(generated_code, namespace_agent)\n",
    "agent_func = namespace_agent['response_function']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define our input and the expected function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = pd.DataFrame({\"col1_a\": [1, 0, 1], \"col1_b\": [0, 1, 0], \"col2_a\": [0, 1, 0], \"col2_b\": [1, 0, 0], \"col2_c\": [0, 0, 1]})\n",
    "\n",
    "def expected_func(input):\n",
    "    result = pd.from_dummies(input, sep=\"_\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compare the results of the two functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Desired result:')\n",
    "print(expected_func(input))\n",
    "print()\n",
    "print('Agent result:')\n",
    "print(agent_func(input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM code creation with RAG and CoALA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by creating a new LLM agent that is augmented by RAG and CoALA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the FAISS vector storage for the retrieval augmented generation and CoALA\n",
    "from llms.rag.faiss import FAISS\n",
    "from llms.rag.coala import CoALA\n",
    "\n",
    "rag = FAISS.create_index_from_texts(\n",
    "    texts=[\"print('Hello Wourld')\", \"None\", \"Irrelevant content\"],\n",
    "    llm_client=client,\n",
    ")\n",
    "\n",
    "code_storage = FAISS.create_index_from_texts(\n",
    "    texts=[\"Question: Print Hello World\\nFinal Answer:def response_function():\\n    print('Hello World')\"],\n",
    "    llm_client=client,\n",
    ")\n",
    "\n",
    "coala_rag = CoALA(docs_vector_store=rag, code_vector_store=code_storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert client initialisation with RAG and CoALA here and call it !!rag_agent!!\n",
    "rag_agent = ReActAgent(client, rag=coala_rag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reminder\n",
    "prompt = \"\"\"How can I convert this one-hot-encoded dataframe: df = pd.DataFrame({\"col1_a\": [1, 0, 1], \"col1_b\": [0, 1, 0], \"col2_a\": [0, 1, 0], \"col2_b\": [1, 0, 0], \"col2_c\": [0, 0, 1]})\n",
    "into a categorical dataframe?\"\"\"\n",
    "\n",
    "generated_code_rag = rag_agent.run(prompt)\n",
    "\n",
    "namespace_rag_agent = {}\n",
    "exec(generated_code_rag, namespace_rag_agent)\n",
    "rag_agent_func = namespace_rag_agent['response_function']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the `input` and `expected_func` stay the same, we can now compare the results of the rag enhanced agent with the expected result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = pd.DataFrame({\"col1_a\": [1, 0, 1], \"col1_b\": [0, 1, 0], \"col2_a\": [0, 1, 0], \"col2_b\": [1, 0, 0], \"col2_c\": [0, 0, 1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Desired result:')\n",
    "print(expected_func(input))\n",
    "print()\n",
    "print('Rag enhanced agent result:')\n",
    "print(rag_agent_func(input))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
