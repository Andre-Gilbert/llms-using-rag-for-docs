{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of RAG architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from llms.clients.gpt import GPTClient\n",
    "from llms.settings import settings\n",
    "from llms.rag.faiss import DistanceMetric\n",
    "from llms.evaluation.code import evaluate_code_generation, ConfigGrid, RAG, RAGRetriever\n",
    "from tests.pandas import TEST_CASES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_texts(folder_paths: list[str]) -> list[str]:\n",
    "    folder_paths = [Path(folder_path) for folder_path in folder_paths]\n",
    "    files = []\n",
    "    for folder_path in tqdm(folder_paths, desc=\"Searching for files\"):\n",
    "        for child in folder_path.iterdir():\n",
    "            if child.is_file():\n",
    "                files.append(child)\n",
    "    texts = []\n",
    "    for file in tqdm(files, desc=\"Reading files\"):\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "            texts.append(content)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_paths = [\"files/pandas/textfiles/textfiles1\", \"files/pandas/textfiles/textfiles2\", \"files/pandas/textfiles/textfiles3\"]\n",
    "texts = get_texts(folder_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_4_client = GPTClient(\n",
    "    client_id=settings.CLIENT_ID,\n",
    "    client_secret=settings.CLIENT_SECRET,\n",
    "    auth_url=settings.AUTH_URL,\n",
    "    api_base=settings.API_BASE,\n",
    "    deployment_id='gpt-4-32k',\n",
    "    max_response_tokens=1000,\n",
    "    temperature=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_grid = ConfigGrid(\n",
    "    llms=[gpt_4_client],\n",
    "    rag=RAG(\n",
    "        retrievers=[\n",
    "            RAGRetriever.NONE,\n",
    "            RAGRetriever.RAG,\n",
    "            RAGRetriever.RAG_AS_TOOL,\n",
    "            RAGRetriever.CoALA,\n",
    "            RAGRetriever.CoALA_AS_TOOL,\n",
    "        ],\n",
    "        distance_metrics=[DistanceMetric.EUCLIDEAN_DISTANCE, DistanceMetric.MAX_INNER_PRODUCT],\n",
    "        num_search_results=[3],\n",
    "        similarity_search_score_thresholds=[0.0],\n",
    "        text_chunk_sizes=[512],\n",
    "        use_weighted_average_of_text_chunks=[True],\n",
    "        texts=texts,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate_code_generation(config_grid=config_grid, test_cases=TEST_CASES, test_name=\"pandas_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./results\"\n",
    "df = pd.read_csv(f\"{path}/pandas_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(kind='bar', x='index', y='accuracy', legend=False)\n",
    "plt.title('Accuracy for solving test cases')\n",
    "plt.xlabel('Id')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(kind='bar', x='index', y='total_cost', legend=False)\n",
    "plt.title('Total cost for solving test cases')\n",
    "plt.xlabel('Id')\n",
    "plt.ylabel('Total Cost in $')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(kind='bar', x='index', y='total_time', legend=False)\n",
    "plt.title('Total time for solving test cases')\n",
    "plt.xlabel('Id')\n",
    "plt.ylabel('Total Time in seconds')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "details_path = df.iloc[0].details_csv_filepath\n",
    "details_df = pd.read_csv(details_path)\n",
    "details_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms-using-rag-for-docs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
